
@incollection{cooper_liquid_2011,
	title = {Liquid {State} {Machines}: {Motivation}, {Theory}, and {Applications}},
	isbn = {978-1-84816-245-7 978-1-84816-277-8},
	shorttitle = {Liquid {State} {Machines}},
	url = {http://www.worldscientific.com/doi/abs/10.1142/9781848162778_0008},
	language = {en},
	urldate = {2023-10-01},
	booktitle = {Computability in {Context}},
	publisher = {IMPERIAL COLLEGE PRESS},
	author = {Maass, Wolfgang},
	collaborator = {Cooper, S Barry and Sorbi, Andrea},
	month = feb,
	year = {2011},
	doi = {10.1142/9781848162778_0008},
	pages = {275--296},
	file = {Maass - 2011 - Liquid State Machines Motivation, Theory, and App.pdf:/Users/giana/Zotero/storage/K5GJE72E/Maass - 2011 - Liquid State Machines Motivation, Theory, and App.pdf:application/pdf},
}

@article{maass_real-time_2002,
	title = {Real-{Time} {Computing} {Without} {Stable} {States}: {A} {New} {Framework} for {Neural} {Computation} {Based} on {Perturbations}},
	volume = {14},
	issn = {0899-7667, 1530-888X},
	shorttitle = {Real-{Time} {Computing} {Without} {Stable} {States}},
	url = {https://direct.mit.edu/neco/article/14/11/2531-2560/6650},
	doi = {10.1162/089976602760407955},
	abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
	language = {en},
	number = {11},
	urldate = {2023-10-01},
	journal = {Neural Computation},
	author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
	month = nov,
	year = {2002},
	pages = {2531--2560},
	file = {Maass et al. - 2002 - Real-Time Computing Without Stable States A New F.pdf:/Users/giana/Zotero/storage/SW55NVV3/Maass et al. - 2002 - Real-Time Computing Without Stable States A New F.pdf:application/pdf},
}

@article{kaiser_scaling_2017,
	title = {Scaling up liquid state machines to predict over address events from dynamic vision sensors},
	volume = {12},
	issn = {1748-3190},
	url = {https://iopscience.iop.org/article/10.1088/1748-3190/aa7663},
	doi = {10.1088/1748-3190/aa7663},
	abstract = {Short-term visual prediction is important both in biology and robotics. It allows us to anticipate upcoming states of the environment and therefore plan more efficiently. In theoretical neuroscience, liquid state machines have been proposed as a biologically inspired method to perform asynchronous prediction without a model. However, they have so far only been demonstrated in simulation or small scale pre-processed camera images. In this paper, we use a liquid state machine to predict over the whole event stream provided by a real dynamic vision sensor (DVS, or silicon retina). Thanks to the event-based nature of the DVS, the liquid is constantly fed with data when an object is in motion, fully embracing the asynchronicity of spiking neural networks. We propose a smooth continuous representation of the event stream for the short-term visual prediction task. Moreover, compared to previous works (2002 Neural Comput. 2525 282–93 and Burgsteiner H et al 2007 Appl. Intell. 26 99–109), we scale the input dimensionality that the liquid operates on by two order of magnitudes. We also expose the current limits of our method by running experiments in a challenging environment where multiple objects are in motion. This paper is a step towards integrating biologically inspired algorithms derived in theoretical neuroscience to real world robotic setups. We believe that liquid state machines could complement current prediction algorithms used in robotics, especially when dealing with asynchronous sensors.},
	language = {en},
	number = {5},
	urldate = {2023-10-28},
	journal = {Bioinspiration \& Biomimetics},
	author = {Kaiser, Jacques and Stal, Rainer and Subramoney, Anand and Roennau, Arne and Dillmann, Rüdiger},
	month = sep,
	year = {2017},
	note = {Publisher: IOP Publishing},
	pages = {055001},
}

@inproceedings{maass_model_2002,
	title = {A {Model} for {Real}-{Time} {Computation} in {Generic} {Neural} {Microcircuits}},
	volume = {15},
	url = {https://proceedings.neurips.cc/paper/2002/hash/6211080fa89981f66b1a0c9d55c61d0f-Abstract.html},
	abstract = {Henry Markram Brain Mind Institute},
	urldate = {2023-10-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
	year = {2002},
	keywords = {LSM, reservoir},
	file = {Full Text PDF:/Users/giana/Zotero/storage/JXGI4AH6/Maass et al. - 2002 - A Model for Real-Time Computation in Generic Neura.pdf:application/pdf},
}

@inproceedings{ivanov_increasing_2021,
	title = {Increasing {Liquid} {State} {Machine} {Performance} with {Edge}-of-{Chaos} {Dynamics} {Organized} by {Astrocyte}-modulated {Plasticity}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/d79c8788088c2193f0244d8f1f36d2db-Abstract.html},
	urldate = {2023-10-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ivanov, Vladimir and Michmizos, Konstantinos},
	year = {2021},
	pages = {25703--25719},
	file = {Full Text PDF:/Users/giana/Zotero/storage/KHEKZP3P/Ivanov e Michmizos - 2021 - Increasing Liquid State Machine Performance with E.pdf:application/pdf},
}

@article{liu_optimizing_2022,
	title = {Optimizing the {Neural} {Structure} and {Hyperparameters} of {Liquid} {State} {Machines} {Based} on {Evolutionary} {Membrane} {Algorithm}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7390},
	url = {https://www.mdpi.com/2227-7390/10/11/1844},
	doi = {10.3390/math10111844},
	abstract = {As one of the important artificial intelligence fields, brain-like computing attempts to give machines a higher intelligence level by studying and simulating the cognitive principles of the human brain. A spiking neural network (SNN) is one of the research directions of brain-like computing, characterized by better biogenesis and stronger computing power than the traditional neural network. A liquid state machine (LSM) is a neural computing model with a recurrent network structure based on SNN. In this paper, a learning algorithm based on an evolutionary membrane algorithm is proposed to optimize the neural structure and hyperparameters of an LSM. First, the object of the proposed algorithm is designed according to the neural structure and hyperparameters of the LSM. Second, the reaction rules of the proposed algorithm are employed to discover the best neural structure and hyperparameters of the LSM. Third, the membrane structure is that the skin membrane contains several elementary membranes to speed up the search of the proposed algorithm. In the simulation experiment, effectiveness verification is carried out on the MNIST and KTH datasets. In terms of the MNIST datasets, the best test results of the proposed algorithm with 500, 1000 and 2000 spiking neurons are 86.8\%, 90.6\% and 90.8\%, respectively. The best test results of the proposed algorithm on KTH with 500, 1000 and 2000 spiking neurons are 82.9\%, 85.3\% and 86.3\%, respectively. The simulation results show that the proposed algorithm has a more competitive advantage than other experimental algorithms.},
	language = {en},
	number = {11},
	urldate = {2023-10-02},
	journal = {Mathematics},
	author = {Liu, Chuang and Wang, Haojie and Liu, Ning and Yuan, Zhonghu},
	month = jan,
	year = {2022},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {evolutionary membrane algorithm, liquid state machines, P systems, spiking neural network, supervised classification},
	pages = {1844},
	file = {Full Text PDF:/Users/giana/Zotero/storage/Y5EBGX3X/Liu et al. - 2022 - Optimizing the Neural Structure and Hyperparameter.pdf:application/pdf},
}

@misc{koralalage_dynamic_2023,
	title = {Dynamic {Training} of {Liquid} {State} {Machines}},
	url = {http://arxiv.org/abs/2302.03506},
	abstract = {Spiking Neural Networks (SNNs) emerged as a promising solution in the field of Artificial Neural Networks (ANNs), attracting the attention of researchers due to their ability to mimic the human brain and process complex information with remarkable speed and accuracy. This research aimed to optimise the training process of Liquid State Machines (LSMs), a recurrent architecture of SNNs, by identifying the most effective weight range to be assigned in SNN to achieve the least difference between desired and actual output. The experimental results showed that by using spike metrics and a range of weights, the desired output and the actual output of spiking neurons could be effectively optimised, leading to improved performance of SNNs. The results were tested and confirmed using three different weight initialisation approaches, with the best results obtained using the Barabasi-Albert random graph method.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Koralalage, Pavithra and Fakeye, Ireoluwa and Machado, Pedro and Smith, Jason and Ihianle, Isibor Kennedy and Yahaya, Salisu Wada and Oikonomou, Andreas and Lotfi, Ahmad},
	month = sep,
	year = {2023},
	note = {arXiv:2302.03506 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/Users/giana/Zotero/storage/V72E72BH/2302.html:text/html;Full Text PDF:/Users/giana/Zotero/storage/D2T8RG56/Koralalage et al. - 2023 - Dynamic Training of Liquid State Machines.pdf:application/pdf},
}

@article{oloughlin_learning_nodate,
	title = {Learning {Rules} and {Topologies} for {Liquid} {State} {Machines}},
	abstract = {As prospect of the next hardware revolution looms, attending to unconventional computing paradigms becomes more important than ever. Recurrent spiking neural networks (RSNNs) are highly suitable for neuromorphic hardware and come ready with an actionable reservoir computing scheme known as the Liquid State Machine (LSM). An LSM employs an RSNN “reservoir” to process input such that meaningful features can be learned with a simple linear readout map trained on reservoir states. LSMs have garnered success in a number of domains, such as speech, image, and even video recognition. However, the hyper-parameter space for reservoir design is prohibitively large and all of these above mentioned successes make use of specialized LSMs, often with significant designand-tuning overhead. Therefore, a pertinent task for the advancement of LSM literature is the development of an increasingly robust playbook for good reservoir design. To this end, we here survey LSMs in their many forms, both prominent and novel, with particular attention to learning rules and topologies. In terms of learning rules, Short Term Synaptic Plasticity (STSP), Spike Timing Dependent Plasticity (STDP), and their combination Long Short-Term Plasticity (LSTP) are investigated, along with non-learning simple Leaky-Integrate-and-Fire (LIF) neurons. For topologies, random, geometric, and small-world are tested. We characterize reservoir quality as a balance between diverse computational efficacy on two input modalities (image and speech) and coherent dynamics in representational state space. In doing so, we aim to distill favorable reservoir attributes and therefore better equip the state-of-the-art for LSMs that may exhibit the sort of versatility we would hope for in our artificial systems. Our findings indicate that the STSP learning rule paired with small-world topology is the most robust in terms of performance and favorable dynamics across input modalities and given other hyperparameter settings. We also observe a tentative relationship of high classification performance with certain representational dynamics in Principal-Component space. A correlation is found in full-dimensional space with the proposed representation-distance measure of coherent separation and performance (around a certain threshold). Therefore in total we conclude that reservoirs with STSP and small-world topologies merit further investigation and moreover that representational dynamics may be a viable option for reservoir quality assessment. Finally, these findings set the stage for proposed future research on adaptive reservoirs, synthetic learning rules, and semantically continuous representational spaces.},
	language = {en},
	author = {O’Loughlin, Ryan},
	file = {O’Loughlin - Learning Rules and Topologies for Liquid State Mac.pdf:/Users/giana/Zotero/storage/D3Z6PV7B/O’Loughlin - Learning Rules and Topologies for Liquid State Mac.pdf:application/pdf},
}

@article{soures_deep_nodate,
	title = {Deep {Liquid} {State} {Machines} with {Neural} {Plasticity} and {On}-{Device} {Learning}},
	language = {en},
	author = {Soures, Nicholas M},
	file = {Soures - Deep Liquid State Machines with Neural Plasticity .pdf:/Users/giana/Zotero/storage/YLQIM4YV/Soures - Deep Liquid State Machines with Neural Plasticity .pdf:application/pdf},
}

@article{auer_p-delta_2023,
	title = {The p-{Delta} {Learning} {Rule} for {Parallel}},
	abstract = {A learning algorithm is presented for circuits consisting of a single layer of perceptrons (= threshold gates, or equivalently gates with a Heaviside activation function). We refer to such circuits as parallel per-ceptrons. In spite of their simplicity, these circuits can compute any boolean function if one views the majority of the binary perceptron outputs as the binary outputs of the parallel perceptron, and they are universal approximators for arbitrary continuous functions with values in [0, 1] if one views the fraction of perceptrons that output 1 as the ana-log output of the parallel perceptron. For a long time one has thought that there exists no competitive learning algorithms for these extremely simple circuits consisting of gates with binary outputs, which also be-came known as committee machines. It is commonly believed that one * Research for this article was partially supported by the ESPRIT Working Group Neu-roCOLT, No. 8556, and the Fonds zur Förderung der wissenschaftlichen Forschung (FWF), Austria, project P12153.},
	author = {Auer, Peter and Burgsteiner, Harald and Maass, Wolfgang},
	month = oct,
	year = {2023},
	file = {Full Text PDF:/Users/giana/Zotero/storage/PI5ATIZ2/Auer et al. - 2023 - The p-Delta Learning Rule for Parallel.pdf:application/pdf},
}

@article{maass_computational_nodate,
	title = {Computational {Models} for {Generic} {Cortical} {Microcircuits}},
	language = {en},
	author = {Maass, Wolfgang and Natschlager, Thomas},
	keywords = {CNS, LSM, SNN},
	file = {Maass e Natschlager - Computational Models for Generic Cortical Microcir.pdf:/Users/giana/Zotero/storage/DCEL4VP5/Maass e Natschlager - Computational Models for Generic Cortical Microcir.pdf:application/pdf},
}
