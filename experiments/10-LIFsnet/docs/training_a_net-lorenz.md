# E\[10, 01\]: Train a LIF network to forecast Lorenz System signals

- [Problem Framing](#problem-framing)
- [Data Generation](#data-generation)
  - [CV split](#cv-split)
- [Network definition](#network-definition)
- [Hooks](#hooks)
- [Training](#training)
  - [Input dataset](#input-dataset)
  - [Optimization parameters](#optimization-parameters)
- [Results](#results)

The following are notes about a training experiment written in [04-training_net-lorenz.py](../flows/04-training_net-lorenz.py)

## Problem Framing
The problem was framed as time series forecasting.

Lorenz system parameters  
* $\sigma$: 12  
* $\rho$: 30  
* $\beta$: 2.7
* length: 10000
* time delay between input and forecasted signal: 10

The generated signal has 3 dimensions on one axis plus time on another axis: ((X,Y,Z), t) = (3, t). 
## Data Generation
The data were generated by a chaotic Lorenz system with the following characteristics. 


### CV split
The dataset was prepared for cross validation according to the following split:
* training: 0.65
* validation: 0.15
* test: 0.2 

This resulted in 3 pairs of tensors $X_{set}$, $y_{set}$, where $set \in \{training, validation, test\}$. 

## Network definition
The estimator adopted is a neural network made up of 4 logical layers, of which two main LIF layers, interspersed with two Fully Connected (torch.Linear) layers, which act as connection matrices. 
The scheme is as follows 
<img src='imgs/exp0904_LIF_arch.png'>

The LIF Layers are made of [Leaky](https://snntorch.readthedocs.io/en/latest/snn.neurons_leaky.html) layers from snnTorch.

The network implements an inner loop, which reproduces the dynamics of the neurons at a lower relative frequency than the general time steps of the program. For each external time step, 10 steps are executed in the internal loop.


The output of the network is a tuple formed by a spike tensor and a membrane potential tensor both relative to the last layer of Leaf.


## Hooks

For the purpose of subsequent analysis, the outputs of all LIF layers were stored. This was done by exploiting the hooks mechanism of pytorch, since storing tensors, internally in the pytorch network class, bickers with autograd.

## Training

Training was performed over 50 epochs

### Input dataset
The input dataset was defined by reshaping the tensors in order to exploit minibatch training.  
A batch size of 50 was used.  

We have the following input tensors, as examples and labels, with relative dimensions:
* `X_train_mb` \[=\] `(n_batches, batch_size, 3)`
* `y_train_mb` \[=\] `(n_batches, batch_size, 3)`


The only trainable parameters are the ones from the Linear layers, namely no parameters from LIF are trained in this exp.

### Optimization parameters

[Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) was chosen as the optimiser, with the following parameters.

[CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss) was chosen as the loss



## Results
Below we can see a plot of the training.

In the same figure at the top we see the Loss, at the bottom we see the average of the weights of the second Linear layer. 
It can be seen that the Loss gradually tends to diverge, probably due to a vanishing gradient.
<img src='../report/LIF-net-Lorenz-loss_and_weights.png'>

For convenience, the plots of the beginning and end of the training are shown. 
Bear in mind that here the signals have been flattened, in other words, the input and output values of the network are seen as they were administered during training. This means that **the inner loop is included**.

<img src='../report/LIF-net-Lorenz_first.png'>
Above the data in the initial part of the training. You can see the network begins to chase the signal. 


<img src='../report/LIF-net-Lorenz_last.png'>
Above the data in the final part of the training. One can observe that at this point the connections are dead, as shown by the plot of the flat membrane potentials.

